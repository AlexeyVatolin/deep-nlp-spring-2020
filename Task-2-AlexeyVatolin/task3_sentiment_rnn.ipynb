{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSJY9TJ3Y41L"
   },
   "source": [
    "## Assignment 2.3: Text classification via RNN (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNX0kuVSY41M"
   },
   "source": [
    "In this assignment you will perform sentiment analysis of the IMDBs reviews by using RNN. An additional goal is to learn high abstactions of the **torchtext** module that consists of data processing utilities and popular datasets for natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M1I8_TmPY41N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAns7mEDY41Q"
   },
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyg2aER3Y41R"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nBxSuG3Y41U"
   },
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, lower=True)\n",
    "LABEL = LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HPz2KwWiY41W",
    "outputId": "ed864aba-f95d-4545-aa04-13c2b51b9086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:09<00:00, 9.01MB/s]\n"
     ]
    }
   ],
   "source": [
    "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
    "trn, vld = train.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4eYMGY0yY41Y",
    "outputId": "00e42d18-9b91-432a-eaa4-d8276b64443e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 35.5 ms, total: 1.31 s\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNyLPKHYY41c"
   },
   "outputs": [],
   "source": [
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzqYEd_kY41g"
   },
   "source": [
    "The vocab.freqs is a collections.Counter object, so we can take a look at the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "He-kmetjY41g",
    "outputId": "99f5730b-0060-4b27-c46e-7160970300e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 225513),\n",
       " ('a', 111704),\n",
       " ('and', 110729),\n",
       " ('of', 101179),\n",
       " ('to', 93530),\n",
       " ('is', 72445),\n",
       " ('in', 63261),\n",
       " ('i', 49429),\n",
       " ('this', 48961),\n",
       " ('that', 46429)]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TWnn4LWQY41l"
   },
   "source": [
    "### Creating the Iterator (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIv99k1eY41l"
   },
   "source": [
    "During training, we'll be using a special kind of Iterator, called the **BucketIterator**. When we pass data into a neural network, we want the data to be padded to be the same length so that we can process them in batch:\n",
    "\n",
    "e.g.\n",
    "\\[ \n",
    "\\[3, 15, 2, 7\\],\n",
    "\\[4, 1\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] -> \\[ \n",
    "\\[3, 15, 2, 7, **0**\\],\n",
    "\\[4, 1, **0**, **0**, **0**\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] \n",
    "\n",
    "If the sequences differ greatly in length, the padding will consume a lot of wasteful memory and time. The BucketIterator groups sequences of similar lengths together for each batch to minimize padding.\n",
    "\n",
    "Complete the definition of the **BucketIterator** object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POLzDA6uY41m"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "        (trn, vld, tst),\n",
    "        batch_sizes=(batch_size, batch_size, batch_size),\n",
    "        sort=True,\n",
    "        sort_key=lambda x: len(x.text), # write your code here\n",
    "        sort_within_batch=False,\n",
    "        device=device,\n",
    "        repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Z1izKhJY41o"
   },
   "source": [
    "Let's take a look at what the output of the BucketIterator looks like. Do not be suprised **batch_first=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ZoGXBnIhY41o",
    "outputId": "fb03ecd3-f97b-4b26-8358-e0e6466d467a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    9,    10,  1280,  ...,    10,     9, 11804],\n",
       "        [  522,    20,   137,  ...,     7,   371, 47404],\n",
       "        [  853,     7,  2148,  ...,     3,     2,   277],\n",
       "        ...,\n",
       "        [    1,     1,     1,  ...,    24,   220,    52],\n",
       "        [    1,     1,     1,  ...,    40,   531,     5],\n",
       "        [    1,     1,     1,  ...,     9,   112,   743]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCbaJF6mY41r"
   },
   "source": [
    "The batch has all the fields we passed to the Dataset as attributes. The batch data can be accessed through the attribute with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dJ9ysGqIY41s",
    "outputId": "d8bc4df0-5070-4542-f9ac-05ece4486f1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'text', 'label'])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZAfzlFxY41w"
   },
   "source": [
    "### Define the RNN-based text classification model (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSLvOQTdY41w"
   },
   "source": [
    "Start simple first. Implement the model according to the shema below.  \n",
    "![alt text](https://miro.medium.com/max/1396/1*v-tLYQCsni550A-hznS0mw.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugcGqSppY41x"
   },
   "outputs": [],
   "source": [
    "class RNNBaseline(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, emb_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # =============================\n",
    "        #      Write code here\n",
    "        # =============================\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim)\n",
    "        self.classification = nn.Linear(hidden_dim, num_classes)\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        # =============================\n",
    "        #      Write code here\n",
    "        # =============================\n",
    "        x = self.embedding(seq)\n",
    "        _, x = self.rnn(x)\n",
    "        x = self.classification(x)\n",
    "        return F.softmax(x, -1).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "VfQAh6fZY41z",
    "outputId": "7b087fff-3fd0-4016-ca05-4a76b3242edb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (embedding): Embedding(201550, 200)\n",
       "  (rnn): GRU(200, 300)\n",
       "  (classification): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 200\n",
    "nh = 300\n",
    "model = RNNBaseline(len(TEXT.vocab), nh, em_sz, len(LABEL.vocab)); model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pt1zVYeMY412"
   },
   "source": [
    "If you're using a GPU, remember to call model.cuda() to move your model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "inuQk3_nY412",
    "outputId": "0210e45f-eff1-4521-9e89-d5c408185a6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (embedding): Embedding(201550, 200)\n",
       "  (rnn): GRU(200, 300)\n",
       "  (classification): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z0kpRxFbY414"
   },
   "source": [
    "### The training loop (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "61N6roupY414"
   },
   "source": [
    "Define the optimization and the loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a76eiUNUY415"
   },
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters()) # your code goes here\n",
    "loss_func = nn.CrossEntropyLoss() # your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1Pww4ioY418"
   },
   "source": [
    "Define the stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0dCNGYDY418"
   },
   "outputs": [],
   "source": [
    "epochs = 20 # your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fGajpZ8GY42A"
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "log_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "grCYyWp3Y42D",
    "outputId": "402c4ba4-81cd-46e6-9c34-b10304efd174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.6855767456284405, Validation Loss: 0.6718965900146355\n",
      "Epoch: 2, Training Loss: 0.5819907411389107, Validation Loss: 0.5329511206028825\n",
      "Epoch: 3, Training Loss: 0.4506173847365553, Validation Loss: 0.5136477596173852\n",
      "Epoch: 4, Training Loss: 0.391089590796589, Validation Loss: 0.4557222862870006\n",
      "Epoch: 5, Training Loss: 0.36087735236561214, Validation Loss: 0.5070311331142814\n",
      "Epoch: 6, Training Loss: 0.3453608222686461, Validation Loss: 0.45424947900287177\n",
      "Epoch: 7, Training Loss: 0.33872245774216897, Validation Loss: 0.45070389115204246\n",
      "Epoch: 8, Training Loss: 0.3335897464604273, Validation Loss: 0.44812590035341554\n",
      "Epoch: 9, Training Loss: 0.3304089866850498, Validation Loss: 0.4524917466155553\n",
      "Epoch: 10, Training Loss: 0.32783904377996487, Validation Loss: 0.463598412729926\n",
      "Epoch: 11, Training Loss: 0.32736551641982836, Validation Loss: 0.49507346370462646\n",
      "Epoch: 12, Training Loss: 0.3249196514596034, Validation Loss: 0.4505924875453367\n",
      "Epoch: 13, Training Loss: 0.3243432297323742, Validation Loss: 0.4515271527787386\n",
      "Epoch: 14, Training Loss: 0.32444218094766575, Validation Loss: 0.45012025383569426\n",
      "Epoch: 15, Training Loss: 0.32342033784319885, Validation Loss: 0.45117091955774924\n",
      "Epoch: 16, Training Loss: 0.32300397438289474, Validation Loss: 0.45170545502234316\n",
      "Epoch: 17, Training Loss: 0.32342099436443217, Validation Loss: 0.45059319497165035\n",
      "Epoch: 18, Training Loss: 0.3229209617541654, Validation Loss: 0.4552340340816369\n",
      "Epoch: 19, Training Loss: 0.32261054054664, Validation Loss: 0.46555549007351116\n",
      "Epoch: 20, Training Loss: 0.32290802754624914, Validation Loss: 0.45009244271254134\n",
      "CPU times: user 4min 12s, sys: 1min 50s, total: 6min 3s\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for i, batch in enumerate(train_iter): \n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)   \n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        global_step = epoch * len(trn) + (i + 1) * batch_size\n",
    "        if i % log_every == 0:\n",
    "            writer.add_scalar('training_loss', loss.item(), global_step)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_iter)\n",
    "    writer.add_scalar('epoch_loss', epoch_loss, global_step)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        preds = model(x) \n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(val_iter)\n",
    "    writer.add_scalar('val_loss', val_loss, global_step)\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGNKLSasdeAn"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/rnn_epoch_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/rnn_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/rnn_val_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMizXBoWY42F"
   },
   "source": [
    "### Calculate performance of the trained model (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJBQaCGioeG4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "u3xR98CAY42G",
    "outputId": "22eba12d-e537-469d-c1cf-876808876e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.84396 \n",
      "precision = 0.8192144925384216 \n",
      "recall = 0.88272 \n",
      "f1 = 0.8497824329007663\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred, y_true = [], []\n",
    "\n",
    "for batch in test_iter:\n",
    "    x = batch.text\n",
    "    y = batch.label\n",
    "\n",
    "    preds = model(x).argmax(axis=1).cpu().numpy()\n",
    "\n",
    "    y_pred.append(preds)\n",
    "    y_true.append(y.cpu().numpy())\n",
    "\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
    "print('accuracy =', accuracy, '\\nprecision =', precision[1], '\\nrecall =', recall[1], '\\nf1 =', f1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ix9PgaqgY42I"
   },
   "source": [
    "Write down the calculated performance\n",
    "\n",
    "### Accuracy: 0.8440\n",
    "### Precision: 0.8192\n",
    "### Recall: 0.8827\n",
    "### F1: 0.8498"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPbIJlyQY42I"
   },
   "source": [
    "### Experiments (10 points)\n",
    "\n",
    "Experiment with the model and achieve better results. You can find advices [here](https://arxiv.org/abs/1801.06146). Implement and describe your experiments in details, mention what was helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byppJsaSY42J"
   },
   "source": [
    "### 1. ?\n",
    "### 2. ?\n",
    "### 3. ?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "task3_sentiment_rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
