{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2. Language modeling.\n",
    "\n",
    "This task is devoted to language modeling. Its goal is to write in PyTorch an RNN-based language model. Since word-based language modeling requires long training and is memory-consuming due to large vocabulary, we start with character-based language modeling. We are going to train the model to generate words as sequence of characters. During training we teach it to predict characters of the words in the training set.\n",
    "\n",
    "\n",
    "\n",
    "## Task 1. Character-based language modeling: data preparation (15 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the language models on the materials of **Sigmorphon 2018 Shared Task**. First, download the Russian datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to set locale category LC_NUMERIC to en_RU.\n",
      "Warning: Failed to set locale category LC_TIME to en_RU.\n",
      "Warning: Failed to set locale category LC_COLLATE to en_RU.\n",
      "Warning: Failed to set locale category LC_MONETARY to en_RU.\n",
      "Warning: Failed to set locale category LC_MESSAGES to en_RU.\n",
      "--2020-03-27 23:39:38--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.244.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.244.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 533309 (521K) [text/plain]\n",
      "Saving to: ‘russian-train-high’\n",
      "\n",
      "russian-train-high  100%[===================>] 520.81K  1.31MB/s    in 0.4s    \n",
      "\n",
      "2020-03-27 23:39:38 (1.31 MB/s) - ‘russian-train-high’ saved [533309/533309]\n",
      "\n",
      "Warning: Failed to set locale category LC_NUMERIC to en_RU.\n",
      "Warning: Failed to set locale category LC_TIME to en_RU.\n",
      "Warning: Failed to set locale category LC_COLLATE to en_RU.\n",
      "Warning: Failed to set locale category LC_MONETARY to en_RU.\n",
      "Warning: Failed to set locale category LC_MESSAGES to en_RU.\n",
      "--2020-03-27 23:39:38--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.244.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.244.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 53671 (52K) [text/plain]\n",
      "Saving to: ‘russian-dev’\n",
      "\n",
      "russian-dev         100%[===================>]  52.41K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2020-03-27 23:39:38 (1.29 MB/s) - ‘russian-dev’ saved [53671/53671]\n",
      "\n",
      "Warning: Failed to set locale category LC_NUMERIC to en_RU.\n",
      "Warning: Failed to set locale category LC_TIME to en_RU.\n",
      "Warning: Failed to set locale category LC_COLLATE to en_RU.\n",
      "Warning: Failed to set locale category LC_MONETARY to en_RU.\n",
      "Warning: Failed to set locale category LC_MESSAGES to en_RU.\n",
      "--2020-03-27 23:39:39--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.244.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.244.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 53514 (52K) [text/plain]\n",
      "Saving to: ‘russian-test’\n",
      "\n",
      "russian-test        100%[===================>]  52.26K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2020-03-27 23:39:39 (1.22 MB/s) - ‘russian-test’ saved [53514/53514]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 (1 points)**\n",
    "All the files contain tab-separated triples ```<lemma>-<form>-<tags>```, where ```<form>``` may contain spaces (*будете соответствовать*). Write a function that loads a list of all word forms, that do not contain spaces.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "валлонский\tваллонскому\tADJ;DAT;NEUT;SG\n",
      "незаконченный\tнезаконченным\tADJ;INS;NEUT;SG\n",
      "истрёпывать\tистрёпывав\tV.CVB;PST\n",
      "личный\tличного\tADJ;ANIM;ACC;MASC;SG\n",
      "серьга\tсерьгам\tN;DAT;PL\n",
      "необоснованный\tнеобоснованным\tADJ;INS;NEUT;SG\n",
      "тютя\tтюти\tN;NOM;PL\n",
      "зарасти\tзаросла\tV;PST;SG;FEM\n",
      "облётывать\tбудете облётывать\tV;FUT;2;PL\n",
      "идеальный\tидеальна\tADJ;FEM;SG;LGSPEC1\n"
     ]
    }
   ],
   "source": [
    "!head russian-train-high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def read_infile(infile):\n",
    "    \"\"\"\n",
    "    == YOUR CODE HERE ==\n",
    "    \"\"\"\n",
    "    with open(infile) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "    words = []\n",
    "    for line in content:\n",
    "        line_split = line.split('\\t')\n",
    "        if len(line_split) == 3:\n",
    "            words.append(line_split[0])\n",
    "            words.append(line_split[1])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 2000 2000\n",
      "валлонский валлонскому незаконченный незаконченным истрёпывать истрёпывав личный личного серьга серьгам\n"
     ]
    }
   ],
   "source": [
    "train_words = read_infile(\"russian-train-high\")\n",
    "dev_words = read_infile(\"russian-dev\")\n",
    "test_words = read_infile(\"russian-test\")\n",
    "print(len(train_words), len(dev_words), len(test_words))\n",
    "print(*train_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 (2 points)** Write a **Vocabulary** class that allows to transform symbols into their indexes. The class should have the method ```__call__``` that applies this transformation to sequences of symbols and batches of sequences as well. You can also use [SimpleVocabulary](https://github.com/deepmipt/DeepPavlov/blob/c10b079b972493220c82a643d47d718d5358c7f4/deeppavlov/core/data/simple_vocab.py#L31) from DeepPavlov. Fit an instance of this class on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "\"\"\"\n",
    "== YOUR CODE HERE ==\n",
    "\"\"\"\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.unk_index = 0\n",
    "        self.begin_index = 1\n",
    "        self.end_index = 2\n",
    "    \n",
    "    def fit(self, sentences):\n",
    "        self.word2index = {'UNK': 0, 'BEGIN': 1, 'END': 2}\n",
    "        word_index = 3\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in self.word2index.keys():\n",
    "                    self.word2index[word] = word_index\n",
    "                    word_index += 1\n",
    "                    \n",
    "    def __call__(self, sentence):\n",
    "        vectorized_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in self.word2index.keys():\n",
    "                vectorized_sentence.append(self.word2index[word])\n",
    "            else:\n",
    "                vectorized_sentence.append(self.unk_index)\n",
    "        return vectorized_sentence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2index)\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.fit([list(x) for x in train_words])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 (2 points)** Write a **Dataset** class, which should be inherited from ```torch.utils.data.Dataset```. It should take a list of words and the ```vocab``` as initialization arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class Dataset(TorchDataset):\n",
    "    \n",
    "    \"\"\"Custom data.Dataset compatible with data.DataLoader.\"\"\"\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns one tensor pair (source and target). The source tensor corresponds to the input word,\n",
    "        with \"BEGIN\" and \"END\" symbols attached. The target tensor should contain the answers\n",
    "        for the language model that obtain these word as input.        \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        == YOUR CODE HERE ==\n",
    "        \"\"\"\n",
    "        example = ['BEGIN'] + list(self.data[index]) + ['END']\n",
    "        example_indexes = torch.tensor(self.vocab(example))\n",
    "        return example_indexes[:-1], example_indexes[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        == YOUR CODE HERE ==\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_words, vocab)\n",
    "dev_dataset = Dataset(dev_words, vocab)\n",
    "test_dataset = Dataset(test_words, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 (3 points)** Use a standard ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with ```batch_size=1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\"\"\"\n",
    "== YOUR CODE HERE ==\n",
    "\"\"\"\n",
    "train_loader = DataLoader(train_dataset, batch_size=1)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([[ 1,  3,  4,  5,  5,  6,  7,  8,  9, 10, 11]]),\n",
      "  tensor([[ 3,  4,  5,  5,  6,  7,  8,  9, 10, 11,  2]])],\n",
      " [tensor([[ 1,  3,  4,  5,  5,  6,  7,  8,  9,  6, 12, 13]]),\n",
      "  tensor([[ 3,  4,  5,  5,  6,  7,  8,  9,  6, 12, 13,  2]])],\n",
      " [tensor([[ 1,  7, 14, 15,  4,  9,  6,  7, 16, 14,  7,  7, 17, 11]]),\n",
      "  tensor([[ 7, 14, 15,  4,  9,  6,  7, 16, 14,  7,  7, 17, 11,  2]])],\n",
      " [tensor([[ 1,  7, 14, 15,  4,  9,  6,  7, 16, 14,  7,  7, 17, 12]]),\n",
      "  tensor([[ 7, 14, 15,  4,  9,  6,  7, 16, 14,  7,  7, 17, 12,  2]])],\n",
      " [tensor([[ 1, 10,  8, 18, 19, 20, 21, 17,  3,  4, 18, 22]]),\n",
      "  tensor([[10,  8, 18, 19, 20, 21, 17,  3,  4, 18, 22,  2]])],\n",
      " [tensor([[ 1, 10,  8, 18, 19, 20, 21, 17,  3,  4,  3]]),\n",
      "  tensor([[10,  8, 18, 19, 20, 21, 17,  3,  4,  3,  2]])],\n",
      " [tensor([[ 1,  5, 10, 16,  7, 17, 11]]),\n",
      "  tensor([[ 5, 10, 16,  7, 17, 11,  2]])],\n",
      " [tensor([[ 1,  5, 10, 16,  7,  6, 23,  6]]),\n",
      "  tensor([[ 5, 10, 16,  7,  6, 23,  6,  2]])],\n",
      " [tensor([[ 1,  8, 14, 19, 22, 23,  4]]),\n",
      "  tensor([[ 8, 14, 19, 22, 23,  4,  2]])],\n",
      " [tensor([[ 1,  8, 14, 19, 22, 23,  4, 12]]),\n",
      "  tensor([[ 8, 14, 19, 22, 23,  4, 12,  2]])]]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list(itertools.islice(train_loader, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5) 1 point** Explain, why this does not work with larger batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Because the examples has different lengths and torch can't merge them into a single matrix. Example of exception listed below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 11 and 12 in dimension 1 at ../aten/src/TH/generic/THTensor.cpp:612",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ebfab80275a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 11 and 12 in dimension 1 at ../aten/src/TH/generic/THTensor.cpp:612"
     ]
    }
   ],
   "source": [
    "next(iter(DataLoader(train_dataset, batch_size=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.6) 5 points** Write a function **collate** that allows you to deal with batches of greater size. See [discussion](https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/8) for an example. Implement your function as a class ```__call__``` method to make it more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(vec, length, dim, pad_symbol):\n",
    "    \"\"\"\n",
    "    Pads a vector ``vec`` up to length ``length`` along axis ``dim`` with pad symbol ``pad_symbol``.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    == YOUR CODE HERE ==\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = length - vec.size(dim)\n",
    "    pad_tensor = torch.zeros(*pad_size, dtype=vec.dtype)\n",
    "    pad_tensor.fill_(pad_symbol)\n",
    "    return torch.cat([vec, pad_tensor], dim=dim)\n",
    "\n",
    "class Padder:\n",
    "    \n",
    "    def __init__(self, dim=0, pad_symbol=0):\n",
    "        self.dim = dim\n",
    "        self.pad_symbol = pad_symbol\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        == YOUR CODE HERE ==\n",
    "        \"\"\"\n",
    "        max_len = max(map(lambda x: x[0].shape[self.dim], batch)) # max len of y will be the same\n",
    "        padded_batch = list(map(lambda x: (pad_tensor(x[0], max_len, self.dim, self.pad_symbol),\n",
    "                                pad_tensor(x[1], max_len, self.dim, self.pad_symbol)), batch))\n",
    "        xs = torch.stack(list(map(lambda x: x[0], padded_batch)), dim=0)\n",
    "        ys = torch.stack(list(map(lambda x: x[1], padded_batch)), dim=0)\n",
    "        return xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.7) 1 points** Again, use ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with the batch size you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\"\"\"\n",
    "== YOUR CODE HERE ==\n",
    "\"\"\"\n",
    "train_loader = DataLoader(train_dataset, collate_fn=Padder(), batch_size=2)\n",
    "dev_loader = DataLoader(dev_dataset, collate_fn=Padder(), batch_size=2)\n",
    "test_loader = DataLoader(test_dataset, collate_fn=Padder(), batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([[ 1,  3,  4,  5,  5,  6,  7,  8,  9, 10, 11,  0],\n",
      "        [ 1,  3,  4,  5,  5,  6,  7,  8,  9,  6, 12, 13]]),\n",
      "  tensor([[ 3,  4,  5,  5,  6,  7,  8,  9, 10, 11,  2,  0],\n",
      "        [ 3,  4,  5,  5,  6,  7,  8,  9,  6, 12, 13,  2]])),\n",
      " (tensor([[ 1,  7, 14, 15,  4,  9,  6,  7, 16, 14,  7,  7, 17, 11],\n",
      "        [ 1,  7, 14, 15,  4,  9,  6,  7, 16, 14,  7,  7, 17, 12]]),\n",
      "  tensor([[ 7, 14, 15,  4,  9,  6,  7, 16, 14,  7,  7, 17, 11,  2],\n",
      "        [ 7, 14, 15,  4,  9,  6,  7, 16, 14,  7,  7, 17, 12,  2]])),\n",
      " (tensor([[ 1, 10,  8, 18, 19, 20, 21, 17,  3,  4, 18, 22],\n",
      "        [ 1, 10,  8, 18, 19, 20, 21, 17,  3,  4,  3,  0]]),\n",
      "  tensor([[10,  8, 18, 19, 20, 21, 17,  3,  4, 18, 22,  2],\n",
      "        [10,  8, 18, 19, 20, 21, 17,  3,  4,  3,  2,  0]])),\n",
      " (tensor([[ 1,  5, 10, 16,  7, 17, 11,  0],\n",
      "        [ 1,  5, 10, 16,  7,  6, 23,  6]]),\n",
      "  tensor([[ 5, 10, 16,  7, 17, 11,  2,  0],\n",
      "        [ 5, 10, 16,  7,  6, 23,  6,  2]])),\n",
      " (tensor([[ 1,  8, 14, 19, 22, 23,  4,  0],\n",
      "        [ 1,  8, 14, 19, 22, 23,  4, 12]]),\n",
      "  tensor([[ 8, 14, 19, 22, 23,  4,  2,  0],\n",
      "        [ 8, 14, 19, 22, 23,  4, 12,  2]])),\n",
      " (tensor([[ 1,  7, 14,  6, 24,  6,  8,  7,  6,  3,  4,  7,  7, 17, 11],\n",
      "        [ 1,  7, 14,  6, 24,  6,  8,  7,  6,  3,  4,  7,  7, 17, 12]]),\n",
      "  tensor([[ 7, 14,  6, 24,  6,  8,  7,  6,  3,  4,  7,  7, 17, 11,  2],\n",
      "        [ 7, 14,  6, 24,  6,  8,  7,  6,  3,  4,  7,  7, 17, 12,  2]])),\n",
      " (tensor([[ 1, 18, 25, 18, 26],\n",
      "        [ 1, 18, 25, 18, 10]]),\n",
      "  tensor([[18, 25, 18, 26,  2],\n",
      "        [18, 25, 18, 10,  2]])),\n",
      " (tensor([[ 1, 15,  4, 19,  4,  8, 18, 10],\n",
      "        [ 1, 15,  4, 19,  6,  8,  5,  4]]),\n",
      "  tensor([[15,  4, 19,  4,  8, 18, 10,  2],\n",
      "        [15,  4, 19,  6,  8,  5,  4,  2]])),\n",
      " (tensor([[ 1,  6, 24,  5, 20, 18, 17,  3,  4, 18, 22,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 24, 13, 27, 14, 18, 14, 28,  6, 24,  5, 20, 18, 17,  3,  4, 18, 22]]),\n",
      "  tensor([[ 6, 24,  5, 20, 18, 17,  3,  4, 18, 22,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [24, 13, 27, 14, 18, 14, 28,  6, 24,  5, 20, 18, 17,  3,  4, 18, 22,  2]])),\n",
      " (tensor([[ 1, 10, 27, 14,  4,  5, 22,  7, 17, 11],\n",
      "        [ 1, 10, 27, 14,  4,  5, 22,  7,  4,  0]]),\n",
      "  tensor([[10, 27, 14,  4,  5, 22,  7, 17, 11,  2],\n",
      "        [10, 27, 14,  4,  5, 22,  7,  4,  2,  0]]))]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(itertools.islice(train_loader, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Character-based language modeling. (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 (5 points)** Write a network that performs language modeling. It should include three layers:\n",
    "1. **Embedding** layer that transforms input symbols into vectors.\n",
    "2. An **RNN** layer that outputs a sequence of hidden states (you may use https://pytorch.org/docs/stable/nn.html#gru).\n",
    "3. A **Linear** layer with ``softmax`` activation that produces the output distribution for each symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embeddings_dim, hidden_size):\n",
    "        super(RNNLM, self).__init__()\n",
    "        \"\"\"\n",
    "        == YOUR CODE HERE ==\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embeddings_dim)\n",
    "        self.rnn = nn.GRU(embeddings_dim, hidden_size)\n",
    "        self.classification = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        \"\"\"\n",
    "        == YOUR CODE HERE ==\n",
    "        \"\"\"\n",
    "        x = self.embedding(inputs)\n",
    "#         print('embedding =', x.shape)\n",
    "        x, _ = self.rnn(x)\n",
    "#         print('rnn =', x.shape)\n",
    "#         print('rnn =', _.shape)\n",
    "        x = self.classification(x)\n",
    "#         print('classification =', x.shape)\n",
    "        return F.softmax(x, dim=-1).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 (1 points)** Write a function ``validate_on_batch`` that takes as input a model, a batch of inputs and a batch of outputs, and the loss criterion, and outputs the loss tensor for the whole batch. This loss should not be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_on_batch(model, criterion, x, y):\n",
    "    \"\"\"\n",
    "    == YOUR CODE HERE ==\n",
    "    \"\"\"\n",
    "    y_pred = model(x)\n",
    "    return criterion(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 (1 points)** Write a function ``train_on_batch`` that accepts all the arguments of ``validate_on_batch`` and also an optimizer, calculates loss and makes a single step of gradient optimization. This function should call ``validate_on_batch`` inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, criterion, x, y, optimizer):\n",
    "    \"\"\"\n",
    "    == YOUR CODE HERE ==\n",
    "    \"\"\"\n",
    "    model.zero_grad()\n",
    "    loss = validate_on_batch(model, criterion, x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 (3 points)** Write a training loop. You should define your ``RNNLM`` model, the criterion, the optimizer and the hyperparameters (number of epochs and batch size). Then train the model for a required number of epochs. On each epoch evaluate the average training loss and the average loss on the validation set. \n",
    "\n",
    "**2.5 (3 points)** Do not forget to average your loss over only non-padding symbols, otherwise it will be too optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "== YOUR CODE HERE ==\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# PARAMS\n",
    "embeddings_dim = 100\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "n_epochs = 5\n",
    "log_every = 1 # log train loss every n batches\n",
    "eval_every = 2_000 # evaluate every n examples\n",
    "pad_symbol = 0\n",
    "\n",
    "# Data\n",
    "train_loader = DataLoader(train_dataset, collate_fn=Padder(pad_symbol=pad_symbol), batch_size=batch_size)\n",
    "dev_loader = DataLoader(dev_dataset, collate_fn=Padder(pad_symbol=pad_symbol), batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, collate_fn=Padder(pad_symbol=pad_symbol), batch_size=batch_size)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "# Model\n",
    "model = RNNLM(len(vocab), embeddings_dim, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0194, 0.0205, 0.0209,  ..., 0.0188, 0.0188, 0.0188],\n",
       "         [0.0162, 0.0183, 0.0168,  ..., 0.0177, 0.0177, 0.0177],\n",
       "         [0.0193, 0.0179, 0.0195,  ..., 0.0152, 0.0152, 0.0152],\n",
       "         ...,\n",
       "         [0.0188, 0.0190, 0.0159,  ..., 0.0157, 0.0157, 0.0157],\n",
       "         [0.0174, 0.0184, 0.0182,  ..., 0.0180, 0.0180, 0.0180],\n",
       "         [0.0222, 0.0201, 0.0171,  ..., 0.0214, 0.0214, 0.0214]],\n",
       "\n",
       "        [[0.0192, 0.0211, 0.0218,  ..., 0.0184, 0.0184, 0.0184],\n",
       "         [0.0151, 0.0183, 0.0154,  ..., 0.0171, 0.0171, 0.0171],\n",
       "         [0.0201, 0.0181, 0.0200,  ..., 0.0138, 0.0138, 0.0138],\n",
       "         ...,\n",
       "         [0.0194, 0.0197, 0.0149,  ..., 0.0149, 0.0149, 0.0149],\n",
       "         [0.0177, 0.0186, 0.0187,  ..., 0.0187, 0.0187, 0.0187],\n",
       "         [0.0231, 0.0205, 0.0158,  ..., 0.0227, 0.0227, 0.0227]],\n",
       "\n",
       "        [[0.0188, 0.0183, 0.0196,  ..., 0.0182, 0.0182, 0.0182],\n",
       "         [0.0145, 0.0159, 0.0195,  ..., 0.0169, 0.0169, 0.0169],\n",
       "         [0.0205, 0.0174, 0.0184,  ..., 0.0131, 0.0131, 0.0131],\n",
       "         ...,\n",
       "         [0.0197, 0.0206, 0.0181,  ..., 0.0146, 0.0146, 0.0146],\n",
       "         [0.0179, 0.0184, 0.0204,  ..., 0.0193, 0.0193, 0.0193],\n",
       "         [0.0233, 0.0187, 0.0170,  ..., 0.0233, 0.0233, 0.0233]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0175, 0.0221, 0.0184,  ..., 0.0181, 0.0181, 0.0181],\n",
       "         [0.0140, 0.0183, 0.0126,  ..., 0.0169, 0.0169, 0.0169],\n",
       "         [0.0207, 0.0175, 0.0200,  ..., 0.0120, 0.0120, 0.0120],\n",
       "         ...,\n",
       "         [0.0196, 0.0204, 0.0177,  ..., 0.0145, 0.0145, 0.0145],\n",
       "         [0.0184, 0.0202, 0.0228,  ..., 0.0207, 0.0207, 0.0207],\n",
       "         [0.0234, 0.0198, 0.0152,  ..., 0.0237, 0.0237, 0.0237]],\n",
       "\n",
       "        [[0.0175, 0.0204, 0.0217,  ..., 0.0181, 0.0181, 0.0181],\n",
       "         [0.0140, 0.0187, 0.0145,  ..., 0.0169, 0.0169, 0.0169],\n",
       "         [0.0207, 0.0179, 0.0210,  ..., 0.0120, 0.0120, 0.0120],\n",
       "         ...,\n",
       "         [0.0196, 0.0194, 0.0187,  ..., 0.0145, 0.0145, 0.0145],\n",
       "         [0.0184, 0.0165, 0.0244,  ..., 0.0207, 0.0207, 0.0207],\n",
       "         [0.0234, 0.0191, 0.0167,  ..., 0.0237, 0.0237, 0.0237]],\n",
       "\n",
       "        [[0.0175, 0.0191, 0.0237,  ..., 0.0181, 0.0181, 0.0181],\n",
       "         [0.0140, 0.0181, 0.0153,  ..., 0.0169, 0.0169, 0.0169],\n",
       "         [0.0207, 0.0179, 0.0205,  ..., 0.0120, 0.0120, 0.0120],\n",
       "         ...,\n",
       "         [0.0196, 0.0194, 0.0191,  ..., 0.0145, 0.0145, 0.0145],\n",
       "         [0.0184, 0.0153, 0.0259,  ..., 0.0207, 0.0207, 0.0207],\n",
       "         [0.0234, 0.0192, 0.0172,  ..., 0.0237, 0.0237, 0.0237]]],\n",
       "       grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_non_padding(loss, y):\n",
    "    mask = (y != pad_symbol).int()\n",
    "    sentence_loss = (loss * mask).sum(axis=0) / mask.sum(axis=0)\n",
    "    return sentence_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.017172813415527: 100%|██████████| 157/157 [00:24<00:00,  6.41it/s] \n",
      "Loss: 4.016729354858398: 100%|██████████| 157/157 [00:23<00:00,  6.65it/s] \n",
      "Loss: 4.020309925079346:  50%|█████     | 79/157 [00:13<00:08,  8.96it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-22a638e74cbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_non_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0meval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a094b82f4fcc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, hidden)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 716\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    717\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    t = tqdm(train_loader, position=0)\n",
    "    for i, (x, y) in enumerate(t):\n",
    "        model.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = average_non_padding(criterion(output, y), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        global_step = (epoch + 1) * len(train_dataset) + (i + 1) * batch_size\n",
    "        if i % log_every == 0:\n",
    "            t.set_description(f\"Loss: {loss.item()}\")\n",
    "            writer.add_scalar('training_loss', loss.item(), global_step)\n",
    "        \n",
    "        if i * batch_size % eval_every < batch_size:\n",
    "            model.eval()\n",
    "            eval_loss = 0\n",
    "            for i, (x, y) in enumerate(dev_loader):\n",
    "                output = model(x)\n",
    "                loss = average_non_padding(criterion(output, y), y)\n",
    "                eval_loss += loss.item()\n",
    "            writer.add_scalar('eval_loss', eval_loss / len(dev_loader), global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 (5 points)** Write a function **predict_on_batch** that outputs letter probabilities of all words in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "== YOUR CODE HERE ==\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7 (1 points)** Calculate the letter probabilities for all words in the test dataset. Print them for 20 last words. Do not forget to disable shuffling in the ``DataLoader``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "== YOUR CODE HERE ==\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.8 (5 points)** Write a function that generates a single word (sequence of indexes) given the model. Do not forget about the hidden state! Be careful about start and end symbol indexes. Use ``torch.multinomial`` for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, max_length=20, start_index=1, end_index=2):\n",
    "    \"\"\"\n",
    "    == YOUR CODE HERE ==\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.9 (1 points)** Use ``generate`` to sample 20 pseudowords. Do not forget to transform indexes to letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    \"\"\"\n",
    "    == YOUR CODE HERE ==\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.10) 5 points** Write a batched version of the generation function. You should sample the following symbol only for the words that are not finished yet, so apply a boolean mask to trace active words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(model, batch_size, max_length = 20, start_index=1, end_index=2):\n",
    "    \"\"\"\n",
    "    == YOUR CODE HERE ==\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = []\n",
    "for _ in range(2):\n",
    "    generated += generate_batch(model, batch_size=10)\n",
    "\"\"\"\n",
    "== YOUR CODE HERE ==\n",
    "\"\"\"\n",
    "for elem in transformed:\n",
    "    print(\"\".join(elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.11) 5 points** Experiment with the type of RNN, number of layers, units and/or dropout to improve the perplexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
